{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load all the EPC files clean and save as csvs\n",
    "\n",
    "# needs pandas and geopandas\n",
    "\n",
    "## Needs UPRN data\n",
    "## uprn = pd.read_csv('./../Data/Ordinance_Survey/osopenuprn_202302.csv')\n",
    "## uprn['UPRN'] = uprn['UPRN'].astype('float64')\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "def load_clean_epc(path_to_epc, path_save_cleaned, columns_keep, UPRN_data):\n",
    "    #list_paths = glob.glob(\"../Data/Energy_Performance_Certificate/all-domestic-certificates/*\")\n",
    "    import glob\n",
    "    import pandas as pd\n",
    "\n",
    "    list_paths = glob.glob(path_to_epc)\n",
    "    \n",
    "    for i in range(len(list_paths)):\n",
    "        path = list_paths[i] + '/certificates.csv'\n",
    "        name = list_paths[i][-20:]\n",
    "        epc = pd.read_csv(path, usecols=columns_keep) #Â columns = ['TOTAL_FLOOR_AREA','UPRN','LODGEMENT_DATE']\n",
    "        # clean the uprn and add\n",
    "        epc['UPRN'] = epc['UPRN'].astype('float64')\n",
    "        epc2 = epc.merge(UPRN_data, on = 'UPRN',how = 'left')\n",
    "        # clean duplicates\n",
    "        epc2['LODGEMENT_DATE'] = pd.to_datetime(epc2['LODGEMENT_DATE'], format='%Y-%m-%d')\n",
    "        epc2 = epc2.sort_values(by=\"LODGEMENT_DATE\").drop_duplicates(subset=[\"UPRN\"], keep=\"last\")\n",
    "        epc2 = epc2.loc[epc2.groupby('UPRN').LODGEMENT_DATE.idxmax()]\n",
    "        # save to new folder\n",
    "        epc2.to_csv(str(path_save_cleaned +  name + '.csv')) # path_save_cleaned = './../Data/Energy_Performance_Certificate/cerificates_processed/'\n",
    "    \n",
    "    return('done')\n",
    "\n",
    "def load_compile_epc(path_to_cleaned):\n",
    "\n",
    "    list_paths2 = glob.glob(path_to_cleaned) #path_to_cleaned = \"./../Data/Energy_Performance_Certificate/cerificates_processed/*.csv\"\n",
    "\n",
    "    data = pd.DataFrame()\n",
    "    for i in range(len(glob.glob(path_to_cleaned))):\n",
    "        x = pd.read_csv(list_paths2[i])\n",
    "        data = pd.concat([data,x])\n",
    "\n",
    "    data = gpd.GeoDataFrame(data, geometry = gpd.points_from_xy(data.LONGITUDE,data.LATITUDE), crs = \"EPSG:4326\")\n",
    "\n",
    "    return(data)\n",
    "\n",
    "\n",
    "\n",
    "def extract_data_from_epc(path_to_certificates, columns, uprn_data_path, save_to_folder):\n",
    "    # load paths\n",
    "    uprn = pd.read_csv(uprn_data_path)\n",
    "    list_paths = glob.glob(path_to_certificates)\n",
    "    original_length = []\n",
    "    cleaned_length = []\n",
    "    name_la = []\n",
    "\n",
    "    # foor loop to extract all the data\n",
    "    for i in range(len(list_paths)):\n",
    "        path = list_paths[i] + '/certificates.csv'\n",
    "        #  define names\n",
    "        name = list_paths[i][-20:]\n",
    "        # \n",
    "        epc = pd.read_csv(path, usecols=columns)\n",
    "        # write length\n",
    "        original_length.append(len(epc))\n",
    "        name_la.append(len(name))\n",
    "        # correct the UPRN\n",
    "        epc['UPRN'] = epc['UPRN'].astype('float64')\n",
    "        # merge the data\n",
    "        epc2 = epc.merge(uprn, left_on = ['UPRN','LOCAL_AUTHORITY'] , right_on = ['UPRN','LAD21CD'],how = 'left')\n",
    "        # drop the duplicates\n",
    "        epc2['LODGEMENT_DATE'] = pd.to_datetime(epc2['LODGEMENT_DATE'], format='%Y-%m-%d')\n",
    "        epc2 = epc2.sort_values(by=\"LODGEMENT_DATE\").drop_duplicates(subset=[\"UPRN\"], keep=\"last\")\n",
    "        epc2 = epc2.loc[epc2.groupby('UPRN').LODGEMENT_DATE.idxmax()]\n",
    "        # measure length\n",
    "        cleaned_length.append(len(epc2))\n",
    "        # write cleaned data\n",
    "        epc2.to_csv(str(save_to_folder + name + '.csv'))\n",
    "    \n",
    "    pd.DataFrame(data = {'original': original_length, 'cleaned':cleaned_length, 'name':name_la}).to_csv(str(save_to_folder + 'lengths_assesment.csv'))\n",
    "    \n",
    "\n",
    "\n",
    "def load_all_cleaned_certificates(list_of_paths):\n",
    "    list_of_paths = glob.glob(list_of_paths)\n",
    "    data = pd.DataFrame()\n",
    "    for i in range(len(list_of_paths)):\n",
    "        x = pd.read_csv(list_of_paths[i])\n",
    "        data = pd.concat([data,x])\n",
    "    return(data) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_paths = glob.glob('./../../Volumes/Extreme_SSD/WORK/EPC/scottish_EPC/certificates/*')\n",
    "\n",
    "uprn = pd.read_csv('./../../Volumes/Extreme_SSD/WORK/MAV/UPRN/osopenuprn_202409.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENERGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_paths)):\n",
    "    path = list_paths[i]\n",
    "    name = list_paths[i][-10:]\n",
    "    epc = pd.read_csv(path,skiprows=[1], usecols=['LODGEMENT_DATE','OSG_REFERENCE_NUMBER','MAINS_GAS_FLAG','ENERGY_CONSUMPTION_CURRENT',\n",
    "                               'CURRENT_ENERGY_EFFICIENCY','CURRENT_ENERGY_RATING','ENERGY_CONSUMPTION_POTENTIAL'])\n",
    "    epc = epc.rename(columns={'OSG_REFERENCE_NUMBER':'UPRN'})\n",
    "    epc['UPRN'] = epc['UPRN'].astype('float64')\n",
    "    epc2 = epc.merge(uprn, on = 'UPRN',how = 'left')\n",
    "    epc2['LODGEMENT_DATE'] = pd.to_datetime(epc2['LODGEMENT_DATE'], format='%Y-%m-%d')\n",
    "    epc2 = epc2.sort_values(by=\"LODGEMENT_DATE\").drop_duplicates(subset=[\"UPRN\"], keep=\"last\")\n",
    "    epc2 = epc2.loc[epc2.groupby('UPRN').LODGEMENT_DATE.idxmax()]\n",
    "    epc2.to_csv(str('./../../Volumes/Extreme_SSD/WORK/EPC/scottish_EPC/cleaned_energy/' +  name ), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_paths)):\n",
    "    path = list_paths[i]\n",
    "    name = list_paths[i][-10:]\n",
    "    epc = pd.read_csv(path,skiprows=[1], usecols=['LODGEMENT_DATE','OSG_REFERENCE_NUMBER','MAINHEAT_DESCRIPTION',\n",
    "                                     'SECONDHEAT_DESCRIPTION','MAINS_GAS_FLAG','NUMBER_OPEN_FIREPLACES'])\n",
    "    epc = epc.rename(columns={'OSG_REFERENCE_NUMBER':'UPRN'})\n",
    "    epc['UPRN'] = epc['UPRN'].astype('float64')\n",
    "    epc2 = epc.merge(uprn, on = 'UPRN',how = 'left')\n",
    "    epc2['LODGEMENT_DATE'] = pd.to_datetime(epc2['LODGEMENT_DATE'], format='%Y-%m-%d')\n",
    "    epc2 = epc2.sort_values(by=\"LODGEMENT_DATE\").drop_duplicates(subset=[\"UPRN\"], keep=\"last\")\n",
    "    epc2 = epc2.loc[epc2.groupby('UPRN').LODGEMENT_DATE.idxmax()]\n",
    "    epc2.to_csv(str('./../../Volumes/Extreme_SSD/WORK/EPC/scottish_EPC/cleaned_heat/' +  name), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_paths)):\n",
    "    path = list_paths[i]\n",
    "    name = list_paths[i][-10:]\n",
    "    epc = pd.read_csv(path, usecols=['LODGEMENT_DATE','OSG_REFERENCE_NUMBER','MAINS_GAS_FLAG','ROOF_DESCRIPTION',\n",
    "                                     'EXTENSION_COUNT','MULTI_GLAZE_PROPORTION','PHOTO_SUPPLY'],skiprows=[1])\n",
    "    epc = epc.rename(columns={'OSG_REFERENCE_NUMBER':'UPRN'})\n",
    "    epc['UPRN'] = epc['UPRN'].astype('float64')\n",
    "    epc2 = epc.merge(uprn, on = 'UPRN',how = 'left')\n",
    "    epc2['LODGEMENT_DATE'] = pd.to_datetime(epc2['LODGEMENT_DATE'], format='%Y-%m-%d')\n",
    "    epc2 = epc2.sort_values(by=\"LODGEMENT_DATE\").drop_duplicates(subset=[\"UPRN\"], keep=\"last\")\n",
    "    epc2 = epc2.loc[epc2.groupby('UPRN').LODGEMENT_DATE.idxmax()]\n",
    "    epc2.to_csv(str('./../../Volumes/Extreme_SSD/WORK/EPC/scottish_EPC/cleaned_loss/' +  name ), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_paths)):\n",
    "    path = list_paths[i]\n",
    "    name = list_paths[i][-10:]\n",
    "    epc = pd.read_csv(path, usecols=['LODGEMENT_DATE','OSG_REFERENCE_NUMBER','MAINS_GAS_FLAG','TOTAL_FLOOR_AREA',\n",
    "                                     'CONSTRUCTION_AGE_BAND','TENURE','BUILT_FORM','PROPERTY_TYPE'],skiprows=[1])\n",
    "    epc = epc.rename(columns={'OSG_REFERENCE_NUMBER':'UPRN'})\n",
    "    epc['UPRN'] = epc['UPRN'].astype('float64')\n",
    "    epc2 = epc.merge(uprn, on = 'UPRN',how = 'left')\n",
    "    epc2['LODGEMENT_DATE'] = pd.to_datetime(epc2['LODGEMENT_DATE'], format='%Y-%m-%d')\n",
    "    epc2 = epc2.sort_values(by=\"LODGEMENT_DATE\").drop_duplicates(subset=[\"UPRN\"], keep=\"last\")\n",
    "    epc2 = epc2.loc[epc2.groupby('UPRN').LODGEMENT_DATE.idxmax()]\n",
    "    epc2.to_csv(str('./../../Volumes/Extreme_SSD/WORK/EPC/scottish_EPC/cleaned_house/' +  name ), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_paths)):\n",
    "    path = list_paths[i] \n",
    "    name = list_paths[i][-10:]\n",
    "    epc = pd.read_csv(path,skiprows=[1], usecols=['LODGEMENT_DATE','OSG_REFERENCE_NUMBER','MAINS_GAS_FLAG', \n",
    "    'PROPERTY_TYPE','TRANSACTION_TYPE','FLOOR_LEVEL'])\n",
    "    epc = epc.rename(columns={'OSG_REFERENCE_NUMBER':'UPRN'})\n",
    "    epc2 = epc.merge(uprn, on = 'UPRN',how = 'left')\n",
    "    epc2['LODGEMENT_DATE'] = pd.to_datetime(epc2['LODGEMENT_DATE'], format='%Y-%m-%d')\n",
    "    epc2 = epc2.sort_values(by=\"LODGEMENT_DATE\").drop_duplicates(subset=[\"UPRN\"], keep=\"last\")\n",
    "    epc2 = epc2.loc[epc2.groupby('UPRN').LODGEMENT_DATE.idxmax()]\n",
    "    epc2.to_csv(str('./../../Volumes/Extreme_SSD/WORK/EPC/scottish_EPC/cleaned_other/' +  name + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
